---
external help file: powershai-help.xml
schema: 2.0.0
powershai: true
---

# Get-AiChat

## SYNOPSIS <!--!= @#Synop !-->
Sends messages to an LLM and returns the response

## DESCRIPTION <!--!= @#Desc !-->
This is the most basic form of Chat promoted by PowershAI.  
With this function, you can send a message to an LLM from the current provider.  

This function is a lower-level, standardized way to access an LLM that PowershAI provides.  
It does not manage history or context. It is useful for invoking simple prompts that do not require multiple interactions like in a Chat. 
Although it supports Function Calling, it does not execute any code and only returns the model's response.



** PROVIDER INFORMATION
	The provider must implement the Chat function for this functionality to be available. 
	The chat function must return an object with the response that follows the same specification as OpenAI's Chat Completion.
	The following links serve as a basis:
		https://platform.openai.com/docs/guides/chat-completions
		https://platform.openai.com/docs/api-reference/chat/object (non-streaming response)
	The provider must implement the parameters of this function. 
	See the documentation for each parameter for details and how to map to a provider;
	
	When the model does not support one of the provided parameters (that is, there is no equivalent functionality, or it cannot be implemented in an equivalent way), an error must be returned.

## SYNTAX <!--!= @#Syntax !-->

```
Get-AiChat [[-prompt] <Object>] [[-temperature] <Object>] [[-model] <Object>] [[-MaxTokens] <Object>] [[-ResponseFormat] <Object>] 
[[-Functions] <Object>] [[-RawParams] <Object>] [[-StreamCallback] <Object>] [-IncludeRawResp] [<CommonParameters>]
```

## PARAMETERS <!--!= @#Params !-->

### -prompt
The prompt to be sent. It must be in the format described by the ConvertTo-OpenaiMessage function

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 1
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -temperature
Model temperature

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 2
Default Value: 0.6
Accept pipeline input: false
Accept wildcard characters: false
```

### -model
Model name. If not specified, uses the provider's default.

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 3
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -MaxTokens
Maximum number of tokens to be returned

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 4
Default Value: 1024
Accept pipeline input: false
Accept wildcard characters: false
```

### -ResponseFormat
Response format 
The acceptable formats and behavior must follow the same as OpenAI: https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format
Shortcuts:
	"json"|"json_object", equivalent to {"type": "json_object"}
	object must specify a schema as if it were passed directly to the OpenAI API, in the response_format.json_schema field

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 5
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -Functions
List of tools that must be invoked!
You can use commands like Get-OpenaiTool*, to easily transform PowerShell functions into the expected format!
If the model invokes the function, the response, both in stream and normal, must also follow the OpenAI tool calling model.
This parameter must follow the same schema as OpenAI's Function Calling: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 6
Default Value: @()
Accept pipeline input: false
Accept wildcard characters: false
```

### -RawParams
Specify direct parameters from the provider's API.
This will overwrite the values that were calculated and generated based on the other parameters.

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 7
Default Value: @{}
Accept pipeline input: false
Accept wildcard characters: false
```

### -StreamCallback
Enables the Stream model 
You must specify a ScriptBlock that will be invoked for each text generated by the LLM.
The script must receive a parameter that represents each piece, in the same streaming format returned
	This parameter is an object that will contain the property choices, which is in the same schema returned by OpenAI's streaming:
		https://platform.openai.com/docs/api-reference/chat/streaming

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 8
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -IncludeRawResp
Include the API response in a field called IncludeRawResp

```yml
Parameter Set: (All)
Type: SwitchParameter
Aliases: 
Accepted Values: 
Required: false
Position: named
Default Value: False
Accept pipeline input: false
Accept wildcard characters: false
```


<!--PowershaiAiDocBlockStart-->
_Automatically translated using PowershAI and AI._
<!--PowershaiAiDocBlockEnd-->
